{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs4 4.6.3\n",
      "sklearn 0.19.1\n",
      "nltk 3.3\n",
      "re 2.2.1\n",
      "requests 2.19.1\n"
     ]
    }
   ],
   "source": [
    "import bs4; print( 'bs4 ' + bs4.__version__)\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "import sklearn; print( 'sklearn ' + sklearn.__version__)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import nltk; print( 'nltk ' + nltk.__version__)\n",
    "from nltk import word_tokenize, pos_tag, RegexpParser;\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, LancasterStemmer\n",
    "\n",
    "# from autocorrect import spell; print('autocorrect 0.3.0')\n",
    "\n",
    "import re; print('re ' + re.__version__)\n",
    "import requests; print('requests ' + requests.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Topic Modeling\n",
    "\n",
    "First we continue where we left off with [Homework 5](https://github.com/kjprice/smu-nlp/blob/e769b93945e6f45ae90aab3ec53b6595ae7bc7da/homework/Homework%205.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code from Homework 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The home page for various movies' reviews, from which will get the links for individual reviews\n",
    "review_home_urls = {\n",
    "    'green_mile': 'https://www.imdb.com/title/tt0120689/reviews?ref_=tt_ql_3',\n",
    "    'forest_gump': 'https://www.imdb.com/title/tt0109830/reviews?ref_=tt_ov_rt',\n",
    "    'cast_away': 'https://www.imdb.com/title/tt0162222/reviews?ref_=tt_ov_rt',\n",
    "    'terminal': 'https://www.imdb.com/title/tt0362227/reviews?ref_=tt_ql_3',\n",
    "    'catch_me_if_you_can': 'https://www.imdb.com/title/tt0264464/reviews?ref_=tt_ql_3',\n",
    "    'road_to_perdition': 'https://www.imdb.com/title/tt0257044/reviews?ref_=tt_ql_3',\n",
    "}\n",
    "def get_text_from_url(url):\n",
    "    return requests.get(url).text\n",
    "text = get_text_from_url(review_home_urls['green_mile'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links_from_html(html):\n",
    "    tags = BeautifulSoup(html, 'html.parser', parse_only=SoupStrainer('a', href=True))\n",
    "    urls = [str(tag.attrs['href']) for tag in tags]\n",
    "    return urls\n",
    "all_links = get_all_links_from_html(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_urls_from_links(links):\n",
    "    url_template = 'https://www.imdb.com{}'\n",
    "    # url_template = 'http://www.gutenberg.org/files/{}/{}-h/{}-h.htm'\n",
    "    return [url_template.format(link) for link in links]\n",
    "\n",
    "urls = get_review_urls_from_links(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevent_link(link):\n",
    "    if '/review/' in link:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_relevent_links(links):\n",
    "    relevent_links = filter(relevent_link, all_links)\n",
    "    unique_relevent_links = set(relevent_links)\n",
    "    return list(unique_relevent_links)\n",
    "relevent_urls = get_relevent_links(urls)\n",
    "len(relevent_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strain_content(name, attrs):\n",
    "    if name == 'div' and dict(attrs).get('class', None) == 'content':\n",
    "        return True\n",
    "    return False\n",
    "def clean_review_text(text):\n",
    "    return re.split('\\\\n\\\\n\\s+\\d+ out of \\d+', text)[0]\n",
    "def get_review_from_url(url):\n",
    "    html = get_text_from_url(url)\n",
    "    tags = BeautifulSoup(html, 'html.parser', parse_only=SoupStrainer(strain_content))\n",
    "    review = clean_review_text(tags.text)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_from_site(url):\n",
    "    reviews = []\n",
    "\n",
    "    reviews_home_text = get_text_from_url(url)\n",
    "    all_links = get_all_links_from_html(reviews_home_text)\n",
    "    relevent_links = get_relevent_links(all_links)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    review_urls = get_review_urls_from_links(relevent_links)\n",
    "    for url in review_urls:\n",
    "        reviews.append(get_review_from_url(url))\n",
    "        # break\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews_from_all_sites():\n",
    "    all_reviews = []\n",
    "    review_titles = review_home_urls.keys()\n",
    "    for title in review_titles:\n",
    "        review_home_url = review_home_urls[title]\n",
    "        all_reviews = all_reviews + get_review_from_site(review_home_url)\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve All Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews = get_reviews_from_all_sites()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe length of the movie was perfect. It kept to the story to an amazing degree. The few changes did'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews[0][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_words(sentences):\n",
    "    return [word_tokenize(sentence) for sentence in sentences]\n",
    "# sentences_to_words([all_reviews[0]])[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boss', '.']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stop_words = ['the', 'green', 'mile', 'shawshank', 'redemption', 'one']\n",
    "stop_words = custom_stop_words + stopwords.words('english')\n",
    "def remove_stop_words(words):\n",
    "    return [word for word in words if word not in stop_words]\n",
    "remove_stop_words(['he', 'her', 'boss', '.', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_only_numbers_and_letters(words):\n",
    "    regex_matcher = '^[a-zA-Z0-9]+$'\n",
    "    return [word for word in words if re.match(regex_matcher, word)]\n",
    "# include_only_numbers_and_letters(['1', 'KJ', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer = PorterStemmer()\n",
    "#stemmer = SnowballStemmer('english')\n",
    "stemmer = LancasterStemmer()\n",
    "def lemmatize_words(words):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "# lemmatize_words(['running', 'fastest', 'hats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['length', 'movie', 'perfect', 'kept', 'story']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_documents(documents):\n",
    "    documents_with_formatted_words = [document.lower().strip() for document in documents]\n",
    "    documents_of_words = sentences_to_words(documents_with_formatted_words)\n",
    "    content_words_and_characters = [remove_stop_words(words) for words in documents_of_words]\n",
    "    content_words = [include_only_numbers_and_letters(characters) for characters in content_words_and_characters]\n",
    "    #lematized_words = [lemmatize_words(words) for words in content_words]\n",
    "    return content_words  \n",
    "preprocess_documents(all_reviews[0:2])[0][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['length', 'movie', 'perfect', 'kept', 'story', 'amazing', 'degree']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_of_words = preprocess_documents(all_reviews)\n",
    "documents_of_words[0][0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17454"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_all_words = [word for document in documents_of_words for word in document]\n",
    "len(list_of_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1330"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique Words\n",
    "len(set(list_of_all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten documents back to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_documents = [' '.join(document) for document in documents_of_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired, in part by https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "NUMBER_OF_FEATURES = 1000\n",
    "NUMBER_OF_TOPICS  = 10\n",
    "NUMBER_OF_TOP_WORDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function \n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic {}:\".format(topic_idx))\n",
    "        stuff = \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        print (stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize (Bag Of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_features=NUMBER_OF_FEATURES)\n",
    "tf = tf_vectorizer.fit_transform(flattened_documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'length movie perfect kept story amazing degree changes hurt feeling telling story stirring captivati'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_documents[0][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=NUMBER_OF_TOPICS, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "movie seen time movies like ever tom hanks hours ca\n",
      "Topic 1:\n",
      "best come like duncan film block hanks michael certainly also\n",
      "Topic 2:\n",
      "paul film movie think michael year coffey death john man\n",
      "Topic 3:\n",
      "film long time however many story emotion feel performance get\n",
      "Topic 4:\n",
      "duncan hanks edgecomb tom paul film percy michael find role\n",
      "Topic 5:\n",
      "men story prison edgecomb king steven stay movie true man\n",
      "Topic 6:\n",
      "movie book great would read see performances king people michael\n",
      "Topic 7:\n",
      "movie cast yet events michael find three prison story depth\n",
      "Topic 8:\n",
      "story book screen film like king length never three darabont\n",
      "Topic 9:\n",
      "make film john paul duncan coffey darabont cast time pain\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf, tf_feature_names, NUMBER_OF_TOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=NUMBER_OF_TOPICS, max_iter=50, learning_method='online', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "stick absolutely condition would shallow either us films teaches effects\n",
      "Topic 1:\n",
      "story film like screen book length king ever world love\n",
      "Topic 2:\n",
      "movie book great story would mind see read movies best\n",
      "Topic 3:\n",
      "never films us products like would human feel quality become\n",
      "Topic 4:\n",
      "story anyone better exactly personally discovery true adds small ostensible\n",
      "Topic 5:\n",
      "movie cast michael long yet performance hanks tom find duncan\n",
      "Topic 6:\n",
      "film duncan story make hanks edgecomb men tom paul michael\n",
      "Topic 7:\n",
      "film cast coffey michael expression production performances john duncan power\n",
      "Topic 8:\n",
      "cast film movie story make paul edgecomb time john emotive\n",
      "Topic 9:\n",
      "movie seen think film paul many time hanks tom hours\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, tf_feature_names, NUMBER_OF_TOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With help from https://shuaiw.github.io/2016/12/22/topic-modeling-and-tsne-visualzation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 150 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 150 samples in 0.002s...\n",
      "[t-SNE] Computed conditional probabilities for sample 150 / 150\n",
      "[t-SNE] Mean sigma: 6.877036\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 54.131004\n",
      "[t-SNE] Error after 1000 iterations: -3.737069\n"
     ]
    }
   ],
   "source": [
    "tsne_lda = tsne_model.fit_transform(tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_topics = lda.components_.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "n_top_words = 5 # number of keywords we show\n",
    "\n",
    "# 20 colors\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lda_keys = []\n",
    "for i in range(X_topics.shape[0]):\n",
    "    _lda_keys +=  X_topics[i].argmax(),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summaries = []\n",
    "topic_word = lda.components_  # all topic words\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words + 1):-1] # get!\n",
    "    topic_summaries.append(' '.join(topic_words)) # append!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BokehUserWarning: ColumnDataSource's columns must be of the same length. Current lengths: ('fill_color', 10), ('x', 150), ('y', 150)\n",
      "BokehUserWarning: ColumnDataSource's columns must be of the same length. Current lengths: ('fill_color', 10), ('line_color', 10), ('x', 150), ('y', 150)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display: table;\"><div style=\"display: table-row;\"><div style=\"display: table-cell;\"><b title=\"bokeh.models.renderers.GlyphRenderer\">GlyphRenderer</b>(</div><div style=\"display: table-cell;\">id&nbsp;=&nbsp;'3000', <span id=\"3003\" style=\"cursor: pointer;\">&hellip;)</span></div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">data_source&nbsp;=&nbsp;ColumnDataSource(id='2997', ...),</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">glyph&nbsp;=&nbsp;Scatter(id='2998', ...),</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">hover_glyph&nbsp;=&nbsp;None,</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_event_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_property_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">level&nbsp;=&nbsp;'glyph',</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">muted&nbsp;=&nbsp;False,</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">muted_glyph&nbsp;=&nbsp;None,</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">name&nbsp;=&nbsp;None,</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">nonselection_glyph&nbsp;=&nbsp;Scatter(id='2999', ...),</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">selection_glyph&nbsp;=&nbsp;None,</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">subscribed_events&nbsp;=&nbsp;[],</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">tags&nbsp;=&nbsp;[],</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">view&nbsp;=&nbsp;CDSView(id='3001', ...),</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">visible&nbsp;=&nbsp;True,</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">x_range_name&nbsp;=&nbsp;'default',</div></div><div class=\"3002\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">y_range_name&nbsp;=&nbsp;'default')</div></div></div>\n",
       "<script>\n",
       "(function() {\n",
       "  var expanded = false;\n",
       "  var ellipsis = document.getElementById(\"3003\");\n",
       "  ellipsis.addEventListener(\"click\", function() {\n",
       "    var rows = document.getElementsByClassName(\"3002\");\n",
       "    for (var i = 0; i < rows.length; i++) {\n",
       "      var el = rows[i];\n",
       "      el.style.display = expanded ? \"none\" : \"table-row\";\n",
       "    }\n",
       "    ellipsis.innerHTML = expanded ? \"&hellip;)\" : \"&lsaquo;&lsaquo;&lsaquo;\";\n",
       "    expanded = !expanded;\n",
       "  });\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "GlyphRenderer(id='3000', ...)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = '20 newsgroups LDA viz'\n",
    "num_example = X_topics.shape[1]\n",
    "\n",
    "plot_lda = bp.figure(plot_width=600, plot_height=400,\n",
    "                     title=title,\n",
    "                     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "                     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_lda.scatter(x=tsne_lda[:, 0], y=tsne_lda[:, 1],\n",
    "                 color=colormap[_lda_keys][:num_example]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp.show(plot_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# we only want to keep the body of the documents!\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "# fetch train and test data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=remove)\n",
    "\n",
    "# a list of 18,846 cleaned news in string format\n",
    "# only keep letters & make them all lower case\n",
    "news = [' '.join(filter(str.isalpha, raw.lower().split())) for raw in\n",
    "        newsgroups_train.data + newsgroups_test.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 18846\n",
      "INFO:lda:vocab_size: 16669\n",
      "INFO:lda:n_words: 1033869\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "/usr/local/lib/python3.6/site-packages/lda/utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:lda:<0> log likelihood: -13121602\n",
      "INFO:lda:<10> log likelihood: -9837055\n",
      "INFO:lda:<20> log likelihood: -9332117\n",
      "INFO:lda:<30> log likelihood: -9174819\n",
      "INFO:lda:<40> log likelihood: -9097726\n",
      "INFO:lda:<50> log likelihood: -9051586\n",
      "INFO:lda:<60> log likelihood: -9018917\n",
      "INFO:lda:<70> log likelihood: -8997343\n",
      "INFO:lda:<80> log likelihood: -8981265\n",
      "INFO:lda:<90> log likelihood: -8968022\n",
      "INFO:lda:<100> log likelihood: -8958949\n",
      "INFO:lda:<110> log likelihood: -8952759\n",
      "INFO:lda:<120> log likelihood: -8946155\n",
      "INFO:lda:<130> log likelihood: -8936368\n",
      "INFO:lda:<140> log likelihood: -8931735\n",
      "INFO:lda:<150> log likelihood: -8928241\n",
      "INFO:lda:<160> log likelihood: -8924050\n",
      "INFO:lda:<170> log likelihood: -8920068\n",
      "INFO:lda:<180> log likelihood: -8913792\n",
      "INFO:lda:<190> log likelihood: -8912534\n",
      "INFO:lda:<200> log likelihood: -8907667\n",
      "INFO:lda:<210> log likelihood: -8904023\n",
      "INFO:lda:<220> log likelihood: -8899085\n",
      "INFO:lda:<230> log likelihood: -8897908\n",
      "INFO:lda:<240> log likelihood: -8897429\n",
      "INFO:lda:<250> log likelihood: -8894424\n",
      "INFO:lda:<260> log likelihood: -8890477\n",
      "INFO:lda:<270> log likelihood: -8887986\n",
      "INFO:lda:<280> log likelihood: -8888428\n",
      "INFO:lda:<290> log likelihood: -8887742\n",
      "INFO:lda:<300> log likelihood: -8884685\n",
      "INFO:lda:<310> log likelihood: -8881977\n",
      "INFO:lda:<320> log likelihood: -8882992\n",
      "INFO:lda:<330> log likelihood: -8882113\n",
      "INFO:lda:<340> log likelihood: -8881306\n",
      "INFO:lda:<350> log likelihood: -8880195\n",
      "INFO:lda:<360> log likelihood: -8878108\n",
      "INFO:lda:<370> log likelihood: -8880065\n",
      "INFO:lda:<380> log likelihood: -8879568\n",
      "INFO:lda:<390> log likelihood: -8878658\n",
      "INFO:lda:<400> log likelihood: -8876740\n",
      "INFO:lda:<410> log likelihood: -8877969\n",
      "INFO:lda:<420> log likelihood: -8876698\n",
      "INFO:lda:<430> log likelihood: -8872567\n",
      "INFO:lda:<440> log likelihood: -8873581\n",
      "INFO:lda:<450> log likelihood: -8873383\n",
      "INFO:lda:<460> log likelihood: -8872778\n",
      "INFO:lda:<470> log likelihood: -8873100\n",
      "INFO:lda:<480> log likelihood: -8874500\n",
      "INFO:lda:<490> log likelihood: -8872554\n",
      "INFO:lda:<499> log likelihood: -8871040\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_topics = 20 # number of topics\n",
    "n_iter = 200 # number of iterations\n",
    "\n",
    "# vectorizer: ignore English stopwords & words that occur less than 5 times\n",
    "cvectorizer = CountVectorizer(min_df=5, stop_words='english')\n",
    "cvz = cvectorizer.fit_transform(news)\n",
    "\n",
    "# train an LDA model\n",
    "lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "X_topics = lda_model.fit_transform(cvz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 18846 samples in 0.080s...\n",
      "[t-SNE] Computed neighbors for 18846 samples in 8.957s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 18846\n",
      "[t-SNE] Computed conditional probabilities for sample 18846 / 18846\n",
      "[t-SNE] Mean sigma: 0.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 89.059113\n",
      "[t-SNE] Error after 1000 iterations: 1.837618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# a t-SNE model\n",
    "# angle value close to 1 means sacrificing accuracy for speed\n",
    "# pca initializtion usually leads to better results \n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "\n",
    "# 20-D -> 2-D\n",
    "tsne_lda = tsne_model.fit_transform(X_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.plotting import save\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "n_top_words = 5 # number of keywords we show\n",
    "\n",
    "# 20 colors\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lda_keys = []\n",
    "for i in range(X_topics.shape[0]):\n",
    "    _lda_keys +=  X_topics[i].argmax(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summaries = []\n",
    "topic_word = lda_model.topic_word_  # all topic words\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "  topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words + 1):-1] # get!\n",
    "  topic_summaries.append(' '.join(topic_words)) # append!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: table;\"><div style=\"display: table-row;\"><div style=\"display: table-cell;\"><b title=\"bokeh.models.renderers.GlyphRenderer\">GlyphRenderer</b>(</div><div style=\"display: table-cell;\">id&nbsp;=&nbsp;'1251', <span id=\"1254\" style=\"cursor: pointer;\">&hellip;)</span></div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">data_source&nbsp;=&nbsp;ColumnDataSource(id='1248', ...),</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">glyph&nbsp;=&nbsp;Scatter(id='1249', ...),</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">hover_glyph&nbsp;=&nbsp;None,</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_event_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_property_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">level&nbsp;=&nbsp;'glyph',</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">muted&nbsp;=&nbsp;False,</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">muted_glyph&nbsp;=&nbsp;None,</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">name&nbsp;=&nbsp;None,</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">nonselection_glyph&nbsp;=&nbsp;Scatter(id='1250', ...),</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">selection_glyph&nbsp;=&nbsp;None,</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">subscribed_events&nbsp;=&nbsp;[],</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">tags&nbsp;=&nbsp;[],</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">view&nbsp;=&nbsp;CDSView(id='1252', ...),</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">visible&nbsp;=&nbsp;True,</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">x_range_name&nbsp;=&nbsp;'default',</div></div><div class=\"1253\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">y_range_name&nbsp;=&nbsp;'default')</div></div></div>\n",
       "<script>\n",
       "(function() {\n",
       "  var expanded = false;\n",
       "  var ellipsis = document.getElementById(\"1254\");\n",
       "  ellipsis.addEventListener(\"click\", function() {\n",
       "    var rows = document.getElementsByClassName(\"1253\");\n",
       "    for (var i = 0; i < rows.length; i++) {\n",
       "      var el = rows[i];\n",
       "      el.style.display = expanded ? \"none\" : \"table-row\";\n",
       "    }\n",
       "    ellipsis.innerHTML = expanded ? \"&hellip;)\" : \"&lsaquo;&lsaquo;&lsaquo;\";\n",
       "    expanded = !expanded;\n",
       "  });\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "GlyphRenderer(id='1251', ...)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = '20 newsgroups LDA viz'\n",
    "num_example = len(X_topics)\n",
    "\n",
    "plot_lda = bp.figure(plot_width=1400, plot_height=1100,\n",
    "                     title=title,\n",
    "                     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "                     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_lda.scatter(x=tsne_lda[:, 0], y=tsne_lda[:, 1],\n",
    "                 color=colormap[_lda_keys][:num_example]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/bokeh/io/saving.py:127: UserWarning: save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\n",
      "  warn(\"save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\")\n",
      "/usr/local/lib/python3.6/site-packages/bokeh/io/saving.py:140: UserWarning: save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\n",
      "  warn(\"save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/kjprice/Library/Projects/smu/nlp/20 newsgroups LDA viz.html'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_coord = np.empty((X_topics.shape[1], 2)) * np.nan\n",
    "for topic_num in _lda_keys:\n",
    "    if not np.isnan(topic_coord).any():\n",
    "        break\n",
    "    topic_coord[topic_num] = tsne_lda[_lda_keys.index(topic_num)]\n",
    "\n",
    "# plot crucial words\n",
    "for i in range(X_topics.shape[1]):\n",
    "    plot_lda.text(topic_coord[i, 0], topic_coord[i, 1], [topic_summaries[i]])\n",
    "\n",
    "# hover tools\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"content\": \"@content - topic: @topic_key\"}\n",
    "\n",
    "# save the plot\n",
    "save(plot_lda, '{}.html'.format(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lda."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
